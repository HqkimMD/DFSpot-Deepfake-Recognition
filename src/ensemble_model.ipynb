{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "from blazeface import FaceExtractor, BlazeFace, VideoReader\n",
    "from architectures import fornet\n",
    "from architectures.fornet import FeatureExtractor\n",
    "from utils import utils\n",
    "from utils.utils import get_transformer\n",
    "from utils.utils import plot_confusion_matrix\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select architecture, device, face policy, face size, frames per video, dataset and provide model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_choices = ['TimmV2', 'TimmV2ST', 'ViT', 'ViTST']\n",
    "choices = {'v2': 'TimmV2', 'v2st': 'TimmV2ST', 'vit': 'ViT', 'vitst': 'ViTST'}\n",
    "device = torch.device(\n",
    "    'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "face_policy = 'scale'\n",
    "face_size = 224\n",
    "frames_per_video = 32\n",
    "\n",
    "dataset = \"ffpp\"\n",
    "net_name = net_choices[0]\n",
    "net_class = getattr(fornet, net_name)\n",
    "model_path = \"../models/\" + dataset + \"_\" + \"v2.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide path to video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = glob('../sample_videos/ffpp/real/**/*.mp4', recursive=True)\n",
    "file_names = []\n",
    "for i in video_paths:\n",
    "    file_names.append(i.split(\"/\")[4])\n",
    "file_names.sort()\n",
    "len(file_names)\n",
    "file_names\n",
    "\n",
    "video_idxs = [1, 3]\n",
    "\n",
    "input_dir = '../sample_videos/ffpp/real/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weightage = [1, 1, 1, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            pred = model(x.to(device)).cpu().numpy().flatten()\n",
    "            score = expit(pred.mean())\n",
    "            scores[model.__class__.__name__] = score\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get path of all models to ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = glob('../models/**/*.pth', recursive=True)\n",
    "models_for_dataset = []\n",
    "for i in model_paths:\n",
    "    if(i.split(\"/\")[2].startswith(dataset)):\n",
    "        models_for_dataset.append(i)\n",
    "models_for_dataset\n",
    "model_paths = models_for_dataset\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weights for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "lim = 0\n",
    "for i in model_paths:\n",
    "    net_name = choices[i.split(\"/\")[2].split(\"_\")[1].split(\".\")[0]]\n",
    "    if(net_name == 'ViTST' or net_name == 'TimmV2ST'):\n",
    "        net_class = getattr(fornet, net_name)\n",
    "        model: FeatureExtractor = net_class().eval().to(device)\n",
    "        model_path = '../models/' + dataset + \\\n",
    "            '_' + i.split('/')[2].split('_')[1]\n",
    "        model.load_state_dict(torch.load(\n",
    "            model_path, map_location='cpu')['net'])\n",
    "        model_list.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = utils.get_transformer(\n",
    "    face_policy, face_size, model_list[0].get_normalizer(), train=False)\n",
    "facedet = BlazeFace().to(device)\n",
    "facedet.load_weights(\"blazeface/blazeface.pth\")\n",
    "facedet.load_anchors(\"blazeface/anchors.npy\")\n",
    "videoreader = VideoReader(verbose=False)\n",
    "\n",
    "\n",
    "def video_read_fn(x): return videoreader.read_frames(\n",
    "    x, num_frames=frames_per_video)\n",
    "\n",
    "\n",
    "face_extractor = FaceExtractor(video_read_fn=video_read_fn, facedet=facedet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = face_extractor.process_videos(\n",
    "    input_dir=input_dir, filenames=file_names, video_idxs=video_idxs)\n",
    "total_videos = len(video_idxs)\n",
    "\n",
    "\n",
    "faces_frames = [frames_per_video *\n",
    "                x for x in range(0, total_videos+1)]   # [0,32,64,96]\n",
    "\n",
    "faces_hc = torch.stack([transf(image=frame['faces'][0])['image']\n",
    "                       for frame in faces if len(frame['faces'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models = Ensemble(model_list).eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for i in range(0, total_videos):  # (0,3) i.e 0,1,2\n",
    "        score = ensemble_models(faces_hc[faces_frames[i]:faces_frames[i+1]])\n",
    "        predictions[input_dir+file_names[video_idxs[i]]] = [score, {'ensemble_score': sum(score.values())}, {\n",
    "            'predicted_class': 'real' if sum(score.values()) < 0.1 else 'fake', 'true_class': input_dir.split(\"/\")[3]}]\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass = []\n",
    "tclass = []\n",
    "res = []  # [   [predicted_class,true_class],    [predicted_class,true_class]     ....  ]\n",
    "for preds in predictions:\n",
    "    predicted_class = predictions[preds][2]['predicted_class']\n",
    "    true_class = predictions[preds][2]['true_class']\n",
    "    res.append([predicted_class, true_class])\n",
    "    pclass.append(predicted_class)\n",
    "    tclass.append(true_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(pclass)):\n",
    "    if(pclass[i] == 'real'):\n",
    "        pclass[i] = 0\n",
    "    elif(pclass[i] == 'fake'):\n",
    "        pclass[i] = 1\n",
    "\n",
    "pclass = torch.Tensor(pclass)\n",
    "\n",
    "for i in range(0, len(tclass)):\n",
    "    if(tclass[i] == 'real'):\n",
    "        tclass[i] = 0\n",
    "    elif(tclass[i] == 'fake'):\n",
    "        tclass[i] = 1\n",
    "tclass = torch.Tensor(tclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((tclass, pclass), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = torch.zeros(2, 2, dtype=torch.int64)\n",
    "for p in stacked:\n",
    "    tl, pl = p.tolist()\n",
    "    cmt[int(tl), int(pl)] = cmt[int(tl), int(pl)] + 1\n",
    "cmt = cmt.detach().cpu().numpy()\n",
    "cmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ('real', 'fake')\n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_confusion_matrix(cmt, names)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
