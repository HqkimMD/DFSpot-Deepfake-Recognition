{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"1\")\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "from blazeface import FaceExtractor, BlazeFace, VideoReader\n",
    "from architectures import fornet\n",
    "from architectures.fornet import FeatureExtractor\n",
    "from utils import utils\n",
    "sys.path.append('..')\n",
    "import torch.nn as nn\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_choices = ['TimmV2','TimmV2ST','ViT','ViTST']\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "face_policy = 'scale'\n",
    "face_size = 224\n",
    "frames_per_video = 32\n",
    "\n",
    "dataset = \"ffpp\"\n",
    "net_name = net_choices[0]\n",
    "net_class = getattr(fornet, net_name)\n",
    "model_path = \"../models/\" + dataset + \"_\"  + \"v2.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"2\")\n",
    "net: FeatureExtractor = net_class().eval().to(device)\n",
    "net.load_state_dict(torch.load(model_path,map_location='cpu')['net'])\n",
    "print(\"3\")\n",
    "\n",
    "transf = utils.get_transformer(face_policy, face_size, net.get_normalizer(), train=False)\n",
    "facedet = BlazeFace().to(device)\n",
    "facedet.load_weights(\"blazeface/blazeface.pth\")\n",
    "facedet.load_anchors(\"blazeface/anchors.npy\")\n",
    "videoreader = VideoReader(verbose=False)\n",
    "video_read_fn = lambda x: videoreader.read_frames(x, num_frames=frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn=video_read_fn,facedet=facedet)\n",
    "print(\"4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['043.mp4',\n",
       " '091.mp4',\n",
       " '150.mp4',\n",
       " '250.mp4',\n",
       " '377.mp4',\n",
       " '488.mp4',\n",
       " '522.mp4',\n",
       " '666.mp4',\n",
       " '777.mp4',\n",
       " '881.mp4',\n",
       " '992.mp4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = glob.glob('../sample_videos/ffpp/real/**/*.mp4', recursive = True)\n",
    "file_names = []\n",
    "for i in video_paths:\n",
    "    file_names.append(i.split(\"/\")[4])\n",
    "file_names.sort()\n",
    "len(file_names)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_idxs=[1,3]\n",
    "\n",
    "input_dir = '../sample_videos/ffpp/real/'\n",
    "\n",
    "\n",
    "faces = face_extractor.process_videos(input_dir=input_dir , filenames=file_names, video_idxs=video_idxs)\n",
    "total_videos = len(video_idxs)\n",
    "\n",
    "\n",
    "faces_frames = [frames_per_video*x for x in range(0,total_videos+1)]   # [0,32,64,96]\n",
    "\n",
    "faces_hc = torch.stack( [ transf(image=frame['faces'][0])['image'] for frame in faces if len(frame['faces'])] )\n",
    "\n",
    "\n",
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for i in range(0,total_videos): #(0,3) i.e 0,1,2\n",
    "        pred = net(faces_hc[faces_frames[i]:faces_frames[i+1]].to(device)).cpu().numpy().flatten()\n",
    "        score = expit(pred.mean())\n",
    "        predictions[input_dir+file_names[video_idxs[i]]] = [round(score,3),'real' if score<0.1 else 'fake']\n",
    "        predictions[input_dir+file_names[video_idxs[i]]] = [round(score,3),{'predicted_class':'real' if score<0.1 else 'fake','true_class':input_dir.split(\"/\")[3]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../sample_videos/ffpp/real/091.mp4': [0.006,\n",
       "  {'predicted_class': 'real', 'true_class': 'real'}],\n",
       " '../sample_videos/ffpp/real/250.mp4': [0.002,\n",
       "  {'predicted_class': 'real', 'true_class': 'real'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(predictions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedClass = res[0][1]['predicted_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.006, {'predicted_class': 'real', 'true_class': 'real'}],\n",
       " [0.002, {'predicted_class': 'real', 'true_class': 'real'}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'real'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/dfdc_v2.pth',\n",
       " '../models/dfdc_v2st.pth',\n",
       " '../models/ffpp_vitst.pth',\n",
       " '../models/celeb_vit.pth',\n",
       " '../models/ffpp_v2.pth',\n",
       " '../models/celeb_v2.pth',\n",
       " '../models/ffpp_vit.pth',\n",
       " '../models/celeb_v2st.pth',\n",
       " '../models/ffpp_v2st.pth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = []\n",
    "models_dir = '../models'\n",
    "model_paths = glob('%s/*.pth'%models_dir)\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f335be53c46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for i in model_paths:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in model_paths:\n",
    "    \n",
    "    net_class = getattr(fornet, net_name)\n",
    "    \n",
    "\t\tb3_model = EffNet(arch)\n",
    "\t\tcheckpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "\t\tb3_model.load_state_dict(checkpoint)\n",
    "\t\tdel checkpoint\n",
    "\t\tmodel_list.append(b3_model)\n",
    "\n",
    "deepware = Ensemble(model_list).eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "\tdef __init__(self, models):\n",
    "\t\tsuper(Ensemble, self).__init__()\n",
    "\t\tself.models = nn.ModuleList(models)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tpreds = []\n",
    "\t\tfor i, model in enumerate(self.models):\n",
    "\t\t\ty = model(x)\n",
    "\t\t\tpreds.append(y)\n",
    "\t\tfinal = torch.mean(torch.stack(preds), dim=0)\n",
    "\t\treturn final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_choices = ['TimmV2','TimmV2ST','ViT','ViTST']\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "face_policy = 'scale'\n",
    "face_size = 224\n",
    "frames_per_video = 32\n",
    "\n",
    "dataset = \"ffpp\"\n",
    "net_name = net_choices[0]\n",
    "net_class = getattr(fornet, net_name)\n",
    "model_path = \"../models/\" + dataset + \"_\"  + \"v2.pth\"\n",
    "\n",
    "\n",
    "print(\"2\")\n",
    "net: FeatureExtractor = net_class().eval().to(device)\n",
    "net.load_state_dict(torch.load(model_path,map_location='cpu')['net'])\n",
    "print(\"3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitd36668ce65ba4bac82369c2852f55dd3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
